{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_read.Read_TFRecords_fcv as read\n",
    "import data_read.Read_TFRecords as read_aug\n",
    "from util_LOSS import *\n",
    "from util_GRAD import *\n",
    "from util_METRIC import *\n",
    "from util_ETC import *\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Confi_Net(object):\n",
    "    def __init__(self, config):\n",
    "        self.cf=config\n",
    "        \n",
    "        # placeholder for training\n",
    "        self.img_b=tf.placeholder(tf.float32, [None, self.cf.height, self.cf.width, 3])  # 240x320\n",
    "        self.dpt_b=tf.placeholder(tf.float32, [None, self.cf.height, self.cf.width, 1])   # 240x320\n",
    "        self.is_train=tf.placeholder(tf.bool)\n",
    "        \n",
    "        # placeholder for noise\n",
    "        #self.noise = tf.placeholder(tf.float32, [None, int(self.cf.height/8)-2, int(self.cf.width/8)-2, 1])\n",
    "        \n",
    "        # resizing for calculating\n",
    "        self.dpt_r1 = tf.image.resize_images(self.dpt_b,  [int(self.cf.height/8), int(self.cf.width/8)])  # 30x40\n",
    "        self.dpt_r2 = tf.image.resize_images(self.dpt_b,  [int(self.cf.height/4), int(self.cf.width/4)])\n",
    "        self.dpt_r3 = tf.image.resize_images(self.dpt_b,  [int(self.cf.height/2), int(self.cf.width/2)])\n",
    "        self.img_r = tf.image.resize_images(self.img_b, [int(self.cf.height/8), int(self.cf.width/8)])\n",
    "        \n",
    "        # inference\n",
    "        self.infer, self.confi1, self.confi2, self.confi3 = self.network(self.vgg(self.img_b), self.img_b, reuse=False, is_train=self.is_train)  \n",
    "        self.infer_size = tf.image.resize_images(self.infer, [int(self.cf.height/1), int(self.cf.width/1)]) # 40x30 -> 80x60  # for save image\n",
    "        \n",
    "        # infersize\n",
    "        self.infer_r1 = tf.image.resize_images(self.infer,  [int(self.cf.height/8), int(self.cf.width/8)])  # 30x40\n",
    "        self.infer_r2= tf.image.resize_images(self.infer,  [int(self.cf.height/4), int(self.cf.width/4)])\n",
    "        \n",
    "        # confidence map\n",
    "        self.confimap_1= tf.maximum(self.confimap(self.confi1), 1 / ((self.cf.height/8)*(self.cf.width/8))) \n",
    "        self.confimap_2= tf.maximum(self.confimap(self.confi2), 1 / ((self.cf.height/4)*(self.cf.width/4))) \n",
    "        self.confimap_3= tf.maximum(self.confimap(self.confi3), 1 / ((self.cf.height/2)*(self.cf.width/2))) \n",
    "       \n",
    "        # resize\n",
    "        self.confimap_1_size = tf.image.resize_images(self.confimap_1, [int(self.cf.height/4), int(self.cf.width/4)]) # 40x30 -> 80x60  # for save image\n",
    "        self.confimap_2_size = tf.image.resize_images(self.confimap_2, [int(self.cf.height/2), int(self.cf.width/2)]) # \n",
    "        self.confimap_3_size = tf.image.resize_images(self.confimap_3, [int(self.cf.height/1), int(self.cf.width/1)]) # \n",
    "\n",
    "        # variance\n",
    "        _, self.var_1 = tf.nn.moments(self.confimap_1, axes=[1, 2])\n",
    "        _, self.var_2 = tf.nn.moments(self.confimap_2, axes=[1, 2])\n",
    "        _, self.var_3 = tf.nn.moments(self.confimap_3, axes=[1, 2])\n",
    "        self.r_var_1 = tf.reduce_mean(self.var_1)\n",
    "        self.r_var_2 = tf.reduce_mean(self.var_2)\n",
    "        self.r_var_3 = tf.reduce_mean(self.var_3)\n",
    "\n",
    "        \n",
    "        # Define Cost\n",
    "        self.l2_cost = L2_img(self.infer, self.dpt_r3)\n",
    "        self.str_cost = strLoss(self.infer, self.dpt_r3, self.cf)\n",
    "        self.scale_inv_cost = scaleInvLoss(self.infer, self.dpt_r3, self.cf)\n",
    "        self.confi1_cost = confiStrLoss(self.infer_r1, self.dpt_r1, self.confimap_1, self.var_1, self.cf)\n",
    "        self.confi2_cost = confiStrLoss(self.infer_r2, self.dpt_r2, self.confimap_2, self.var_2, self.cf)\n",
    "        self.confi3_cost = confiStrLoss(self.infer, self.dpt_r3, self.confimap_3, self.var_3, self.cf)\n",
    "        #self.confi4_cost = confiLoss(self.infer, self.dpt_r, self.confimap_4, self.var_4, self.cf)\n",
    "        self.ccc_cost = CCC_img(self.infer, self.dpt_r3, self.cf)\n",
    "        \n",
    "        # weight decay\n",
    "        self._vars = tf.trainable_variables()\n",
    "        self.regul = tf.add_n([tf.nn.l2_loss(v) for v in self._vars if 's1' in v.name]) * 0.00001\n",
    "\n",
    "        # TOTAL LOSS\n",
    "        self.confis_avg_cost = (self.confi1_cost + self.confi2_cost + self.confi3_cost)/3\n",
    "        self.Loss =self.regul  + self.l2_cost*0.3 + self.ccc_cost*0.7 + self.confis_avg_cost*0.3\n",
    "\n",
    "        # vgg_var\n",
    "        self.restore_vgg_var = slim.get_variables_to_restore(include=['vgg_16/conv1', 'vgg_16/conv2', 'vgg_16/conv3'])\n",
    "\n",
    "        # Optimizer\n",
    "        self._train_op = tf.train.MomentumOptimizer(5e-4, self.cf.momentum)\n",
    "        self.train_var = bring_var_include_prefix('s1') # + bring_var_include_prefix('vgg_16')\n",
    "        self.train_op = slim.learning.create_train_op(self.Loss, self._train_op, update_ops=self.train_var)  # option :  update_ops = train_var\n",
    "        self.mv_avg = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        self.updates = tf.group(*self.mv_avg)\n",
    "        self.Loss = control_flow_ops.with_dependencies([self.updates], self.Loss)\n",
    "\n",
    "        # setting\n",
    "        self.last_e = tf.Variable(0)\n",
    "        \n",
    "        # placeholder for metric\n",
    "        self._targets = tf.placeholder(tf.float32, [None, self.cf.height, self.cf.width, 1])\n",
    "        self.targets = tf.image.resize_images(self._targets, [120, 160])\n",
    "        self.predictions = tf.placeholder(tf.float32, [None, 120, 160, 1])\n",
    "        \n",
    "        ###METRIC###\n",
    "        self.absRel = AbsRel_img(self.predictions, self.targets)\n",
    "        self.sqrRel = SqrRel_img(self.predictions, self.targets)\n",
    "        self.rmse = RMSE_img(self.predictions, self.targets)\n",
    "        self.ccc = CCC_img(self.predictions, self.targets, self.cf)\n",
    "        self.str = strLoss(self.predictions , self.targets, self.cf)\n",
    "        self.l2 = L2_img(self.predictions, self.targets)\n",
    "        \n",
    "        # sess.run list\n",
    "        self.metric_var = [self.absRel, self.sqrRel, self.rmse, self.ccc, self.str, self.l2]\n",
    "        self.opt_var = [self.train_op, self.Loss, self.infer, self.confis_avg_cost, self.confi1_cost, self.confi2_cost, self.confi3_cost, self.r_var_1, self.r_var_2, self.r_var_3]\n",
    "        self.test_var = [self.Loss, self.infer, self.confis_avg_cost, self.confi1_cost, self.confi2_cost, self.confi3_cost, self.r_var_1, self.r_var_2, self.r_var_3,\n",
    "                         self.infer_size, self.confimap_1_size, self.confimap_2_size, self.confimap_3_size]\n",
    "        \n",
    "       \n",
    "    def confimap(self, confi):\n",
    "        try:\n",
    "            size = confi.get_shape().as_list()\n",
    "        except:\n",
    "            size = confi.shape\n",
    "        b = self.cf.batch_size\n",
    "        h = size[1]\n",
    "        w = size[2]\n",
    "        n = h*w # size of image pixels \n",
    "        \n",
    "        _confi = tf.square(confi)\n",
    "        \n",
    "        m = tf.reduce_mean(_confi, axis=(1,2))  # mean\n",
    "        temp=(tf.reshape(_confi, (b, n))) / m\n",
    "        \n",
    "        return tf.reshape(temp, (b, h, w, 1))\n",
    "              \n",
    "    def vgg(self, inputs, scope='vgg_16'):\n",
    "        with tf.variable_scope('vgg_16') as sc:\n",
    "            with slim.arg_scope([slim.conv2d, slim.fully_connected, slim.max_pool2d]):\n",
    "                net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "                net = slim.max_pool2d(net, [2, 2], scope='pool1')                                   #1/2   (160x120)\n",
    "                net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "                net = slim.max_pool2d(net, [2, 2], scope='pool2')                                  #1/4   (80x60)\n",
    "                net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')  \n",
    "                net = slim.max_pool2d(net, [2, 2], scope='pool3')                                 #1/8   (40x30)\n",
    "                net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "                return net        \n",
    "        \n",
    "    def network(self, vgg, img, reuse=None, is_train=None):\n",
    "        with tf.variable_scope('s1') as scope:\n",
    "            if self.cf.reuse:\n",
    "                scope.reuse_variables()\n",
    "            w=self.cf.width\n",
    "            h=self.cf.height\n",
    "            p=self.cf.padding\n",
    "            w_init = tf.random_normal_initializer(0., 0.02)\n",
    "            \n",
    "            with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, weights_initializer=w_init, normalizer_fn=slim.batch_norm):\n",
    "                with slim.arg_scope([slim.batch_norm], decay=0.9, center=True, scale=True, updates_collections=None, is_training = is_train): \n",
    "                    \n",
    "                    \n",
    "                    # image\n",
    "                    img1 = tf.image.resize_images(img, [int(cf.height/4), int(cf.width/4)])\n",
    "                    img2 = tf.image.resize_images(img, [int(cf.height/2), int(cf.width/2)])\n",
    "                    \n",
    "                    net = slim.repeat(vgg, 6, slim.conv2d, 512, [3, 3], scope='size1')\n",
    "                    cv1 = slim.conv2d(net, 256, [1, 1], scope='cv1')\n",
    "                    \n",
    "                    resize1 = tf.image.resize_images(cv1, [int(cf.height/4), int(cf.width/4)])                                        \n",
    "                    img_feat1 = slim.repeat(img1, 3, slim.conv2d, 64, [3, 3], scope='img_feat1')\n",
    "                    concat1 = tf.concat([img_feat1, resize1], axis=3)\n",
    "                    \n",
    "                    net = slim.repeat(concat1, 2, slim.conv2d, 128, [3, 3], scope='size2')\n",
    "                    net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope='size2_')\n",
    "                    net = slim.repeat(net, 2, slim.conv2d, 384, [3, 3], scope='size2__')\n",
    "                    cv2 = slim.conv2d(net, 128, [1, 1], scope='cv2')\n",
    "                    \n",
    "                    resize2 = tf.image.resize_images(cv2, [int(cf.height/2), int(cf.width/2)])\n",
    "                    img_feat2 = slim.repeat(img2, 3, slim.conv2d, 64, [3, 3], scope='img_feat2')\n",
    "                    concat2 = tf.concat([img_feat2, resize2], axis=3)\n",
    "                    \n",
    "                    net = slim.repeat(concat2, 4, slim.conv2d, 128, [3, 3], scope='size3')\n",
    "                    cv3 = slim.conv2d(net, 64, [1, 1], scope='cv3')\n",
    "                    \n",
    "                    out = slim.conv2d(cv3, 1, [3, 3], activation_fn=None, scope='out')\n",
    "        \n",
    "                    confi1 = slim.conv2d(cv1, 1, [3, 3], padding='VALID', activation_fn=None, normalizer_fn=None, scope='confi1')\n",
    "                    confi2 = slim.conv2d(cv2, 1, [3, 3], padding='VALID', activation_fn=None, normalizer_fn=None, scope='confi2')\n",
    "                    confi3 = slim.conv2d(cv3, 1, [3, 3], padding='VALID', activation_fn=None, normalizer_fn=None, scope='confi3')\n",
    "                                        \n",
    "                    return out, confi1, confi2, confi3\n",
    "    \n",
    "    def training(self):\n",
    "        # training data and test data.        \n",
    "        self.img_batch, self.dpt_batch = read.raw_resized_inputs(self.cf.data_dir, self.cf.batch_size)\n",
    "        self.test_in, self.target = read.test_inputs(self.cf.data_dir, self.cf.batch_size)       \n",
    "        \n",
    "        # placeholder for visuailizing\n",
    "        self.target_img = self.target #tf.image.resize_images(self.target, [int(cf.height/1), int(cf.width/1)]) # target depth image (in TEST time)\n",
    "        \n",
    "        # setting\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        global_step = 0\n",
    "        \n",
    "        # vgg load\n",
    "        restore = tf.train.Saver(self.restore_vgg_var)\n",
    "        restore.restore(sess, os.path.join(cf.vgg_dir, 'vgg_16.ckpt'))\n",
    "        print (\"restore vgg weights\")\n",
    "        \n",
    "        #saver\n",
    "        saver = tf.train.Saver(max_to_keep=20)\n",
    "        ckpt_check_and_load('./ckpt_dir/fuck/confi/paper/', saver, sess)\n",
    "        #load('./ckpt_dir/fuck/confi/fully/' ,saver, sess)\n",
    "        #saver.restore(sess, './ckpt_dir/fuck/confi/paper/s1_weights-5929')\n",
    "        \n",
    "        # ready\n",
    "        start_from = sess.run(self.last_e)\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess, coord)\n",
    "        \n",
    "        # training start\n",
    "        try:\n",
    "            while not coord.should_stop():\n",
    "                for epoch in range(start_from, 1001):\n",
    "                    #print (\"start epoch:\", epoch)            \n",
    "                    total_batch = int(self.cf.num_examples / self.cf.batch_size)\n",
    "                    absrel=0; sqrrel=0; rmse=0; ccc=0; l2=0; loss=0; structure=0;  # METRIC\n",
    "                    confi1=0; var1=0; confi2=0; var2=0; confi3=0; var3=0; # CONFIS\n",
    "                    loss=0; confi_avg_loss=0;  # TOTAL LOSS\n",
    "                    \n",
    "                    for i in range(total_batch):\n",
    "                        #random crop img\n",
    "                        imgs, dpts = sess.run([self.img_batch, self.dpt_batch])\n",
    "                        imgs=np.array(imgs, dtype=np.float32); dpts=np.array(dpts, dtype=np.float32) # type converting\n",
    "                        #noises = np.random.normal(size=[self.cf.batch_size, int(self.cf.height/8)-2, int(self.cf.width/8)-2, 1], loc=0., scale=0.1)\n",
    "                        \n",
    "                        _, losses, infers, confis_avg_loss, confis1, confis2, confis3, vars1, vars2, vars3 = sess.run(self.opt_var, feed_dict={self.img_b:imgs, self.dpt_b:dpts,  self.is_train:True})  # learn\n",
    "                        \n",
    "                        absrel_c, sqrrel_c, rmse_c, ccc_c, str_c, l2_c = sess.run(self.metric_var, feed_dict={self._targets:dpts, self.predictions:infers}) # metric\n",
    "\n",
    "                        global_step +=1  # global count\n",
    "\n",
    "                        #CALCULATE\n",
    "                        loss+=losses/total_batch; confi_avg_loss+=confis_avg_loss/total_batch # TOTAL LOSS\n",
    "                        confi1+=confis1/total_batch; confi2+=confis2/total_batch; confi3+=confis3/total_batch;  # CONFIS\n",
    "                        var1+=vars1/total_batch; var2+=vars2/total_batch; var3+=vars3/total_batch;  # VARS\n",
    "                        ccc+=ccc_c/total_batch; absrel+=absrel_c/total_batch; sqrrel+=sqrrel_c/total_batch; rmse+=rmse_c/total_batch; structure+=str_c/total_batch; l2+=l2_c/total_batch # METRICS                \n",
    "\n",
    "                    if epoch==0 or epoch % 10 == 0 :\n",
    "                        print (\"\")\n",
    "                        print (\"epoch:\", epoch, \"loss: %.3f, ccc: %.3f, absrel: %.3f, sqrrel: %.3f, rmse: %.3f, str1:%.3f, l2: %.3f\"\n",
    "                              % (loss, ccc,  absrel, sqrrel, rmse, structure, l2))\n",
    "                        print (\"[AVG_CONFI] : %.3f\" % (confi_avg_loss))\n",
    "                        print (\"[[confis] (1): %.3f, (2): %.3f, (3): %.3f, [<vars>] (1): %.3f, (2): %.3f, (3): %.3f \" % (confi1, confi2, confi3, var1, var2, var3 ))\n",
    "\n",
    "                    if epoch==0 or epoch % 30 ==0:\n",
    "                        save('./ckpt_dir/fuck/confi/paper', 's1_weights', saver, sess, global_step) # save\n",
    "                        self.test(saver, sess, self.cf, is_training = False) # testr\n",
    "\n",
    "                    sess.run(self.last_e.assign(epoch + 1)) # up-counting\n",
    "                break\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print (\"oorE\")\n",
    "        finally:\n",
    "            print (\"Finally\")\n",
    "            coord.request_stop()        \n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "        \n",
    "    def test(self, saver, sess, config, is_training):\n",
    "        # loading\n",
    "        load('./ckpt_dir/fuck/confi/paper/', saver, sess)\n",
    "\n",
    "        total_batch = int(self.cf.num_examples_test / self.cf.batch_size)\n",
    "        absrel=0; sqrrel=0; rmse=0; ccc=0; l2=0; loss=0; structure=0;  # METRIC\n",
    "        confi1=0; var1=0; confi2=0; var2=0; confi3=0; var3=0; # CONFIS\n",
    "        loss=0; confi_avg_loss=0;  # TOTAL LOSS\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            # input, dpts(=gt_d), binarys(=gt_b), reshaped_dpt(for visual), reshaped_bi(for visual)\n",
    "            imgs, dpts, tar_imgs = sess.run([self.test_in, self.target, self.target_img]) \n",
    "            imgs=np.array(imgs, dtype=np.float32); dpts=np.array(dpts, dtype=np.float32)  # test input  ; # depth target    \n",
    "            \n",
    "            #noises= np.random.normal(size=[self.cf.batch_size, int(self.cf.height/8)-2, int(self.cf.width/8)-2, 1], loc=0., scale=0.1)\n",
    "            \n",
    "            losses, infers, confis_avg_loss, confis1, confis2, confis3,  vars1, vars2, vars3, infer_sizes, confi1_sizes, confi2_sizes, confi3_sizes = sess.run(self.test_var, feed_dict={self.img_b:imgs, self.dpt_b:dpts, self.is_train:False})  # learn\n",
    "            \n",
    "            absrel_c, sqrrel_c, rmse_c, ccc_c, str_c, l2_c = sess.run(self.metric_var, feed_dict={self._targets:dpts, self.predictions:infers}) # metric\n",
    "\n",
    "            #CALCULATE\n",
    "            loss+=losses/total_batch; confi_avg_loss+=confis_avg_loss/total_batch # TOTAL LOSS\n",
    "            confi1+=confis1/total_batch; confi2+=confis2/total_batch; confi3+=confis3/total_batch;# CONFIS\n",
    "            var1+=vars1/total_batch; var2+=vars2/total_batch; var3+=vars3/total_batch; # VARS\n",
    "            ccc+=ccc_c/total_batch; absrel+=absrel_c/total_batch; sqrrel+=sqrrel_c/total_batch; rmse+=rmse_c/total_batch; structure+=str_c/total_batch; l2+=l2_c/total_batch # METRICS      \n",
    "\n",
    "            if i == total_batch-1:\n",
    "                print (\"\")\n",
    "                print (\"<TEST> loss: %.3f, ccc: %.3f, absrel: %.3f, sqrrel: %.3f, rmse: %.3f, str1:%.3f, l2: %.3f\"\n",
    "                      % (loss, ccc,  absrel, sqrrel, rmse, structure, l2))\n",
    "                print (\"[AVG_CONFI] : %.3f\" % (confi_avg_loss))\n",
    "                print (\"[confis] (1): %.3f, (2): %.3f, (3): %.3f, [<vars>] (1): %.3f, (2): %.3f, (3): %.3f \" % (confi1, confi2, confi3,  var1, var2, var3))\n",
    "            \n",
    "\n",
    "         # save_img\n",
    "            if i % 30 ==0:\n",
    "                b=infer_sizes.shape[0]\n",
    "\n",
    "                #reshape (esti, tar)\n",
    "                out_s = np.reshape(infer_sizes,  (b, int(self.cf.height/1), int(self.cf.width/1))) # infer\n",
    "                tar_d = np.reshape(tar_imgs, (b, int(self.cf.height/1), int(self.cf.width/1)))  # depth target\n",
    "                cf1 = np.reshape(confi1_sizes, (b, int(self.cf.height/4), int(self.cf.width/4)))\n",
    "                cf2 = np.reshape(confi2_sizes, (b, int(self.cf.height/2), int(self.cf.width/2)))\n",
    "                cf3 = np.reshape(confi3_sizes, (b, int(self.cf.height/1), int(self.cf.width/1)))\n",
    "\n",
    "                # save\n",
    "                l_epch = str(sess.run(self.last_e))\n",
    "                i=str(i)\n",
    "                for img_idx in range(b):\n",
    "                    idx=str(img_idx)\n",
    "                    plt.imsave('./img_save/during_train/fuck/paper/temp/'+'['+l_epch+'_'+i+']_'+idx+'_Input.jpg', imgs[img_idx])\n",
    "                    plt.imsave('./img_save/during_train/fuck/paper/temp/'+'['+l_epch+'_'+i+']_'+idx+'_D_target.jpg', tar_d[img_idx])\n",
    "                    plt.imsave('./img_save/during_train/fuck/paper/temp/'+'['+l_epch+'_'+i+']_'+idx+'_D_infer.jpg', out_s[img_idx])\n",
    "                    plt.imsave('./img_save/during_train/fuck/paper/temp/'+'['+l_epch+'_'+i+']_'+idx+'_C_1.jpg', cf1[img_idx])\n",
    "                    plt.imsave('./img_save/during_train/fuck/paper/temp/'+'['+l_epch+'_'+i+']_'+idx+'_C_2.jpg', cf2[img_idx])\n",
    "                    plt.imsave('./img_save/during_train/fuck/paper/temp/'+'['+l_epch+'_'+i+']_'+idx+'_C_3.jpg', cf3[img_idx])\n",
    "                    \n",
    "    def load_weights(self, sess, name):\n",
    "        \n",
    "        restore_enc_var = slim.get_variables_to_restore(include=['s1/'])\n",
    "        restore_enc = tf.train.Saver(restore_enc_var)\n",
    "        restore_enc.restore(sess, name)\n",
    "        \n",
    "        print (\"restore 's1' weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config(object):\n",
    "    def __init__(self):\n",
    "        self.vgg_dir = './vgg_w'\n",
    "        self.data_dir='data_read/data/fcv'\n",
    "        self.batch_size= 2\n",
    "        self.is_training=True\n",
    "        self.reuse=None\n",
    "        self.gan=True\n",
    "        self.height=240\n",
    "        self.width=320\n",
    "        self.padding='SAME'\n",
    "        self.momentum=0.9 # 이게 문제가 될수도..\n",
    "        self.epoch_size =501\n",
    "        self.num_examples=(795)\n",
    "        self.num_examples_test=654\n",
    "        self.ckpt_dir ='./ckpt_dir'\n",
    "        self.state='s1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf=config()\n",
    "CN = Confi_Net(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CN.training()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
