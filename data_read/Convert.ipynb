{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "version of TF :  1.0.1\n"
     ]
    }
   ],
   "source": [
    "from utility import *\n",
    "import argparse\n",
    "import sys\n",
    "from imgaug import augmenters as ima\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shelve import open\n",
    "def shelf(name, dic):\n",
    "    shelf = open(name)\n",
    "    shelf['images'] = dic['images']\n",
    "    shelf['depths'] = dic['depths']\n",
    "    shelf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list = tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to(d_data_sets, name):\n",
    "    \"\"\"Convert a data_set to tfrecords. \"\"\"\n",
    "    images = d_data_sets['images']\n",
    "    dpt_images = d_data_sets['depths'] # depth images\n",
    "    \n",
    "    if name is not 'test':\n",
    "        assert dpt_images.shape[0] == 795 #FLAGS.num_train\n",
    "    else :\n",
    "        assert dpt_images.shape[0] == 654 #FLAGS.num_test\n",
    "    \n",
    "    num_examples = images.shape[0]\n",
    "    w = images.shape[2]\n",
    "    h = images.shape[3]\n",
    "    ch = images.shape[1]\n",
    "    \n",
    "    filename = os.path.join('./data', name + '.tfrecords') # dir path for writing\n",
    "    print (\"Writing \", filename)\n",
    "    writer = tf.python_io.TFRecordWriter(filename) # make writer\n",
    "    for index in range(num_examples):\n",
    "        image_raw = images[index].tostring() # extract data to fill example protocol buffer\n",
    "        depth_raw = dpt_images[index].tostring() \n",
    "        # Example protocol buffer\n",
    "        example = tf.train.Example(features = tf.train.Features(feature={\n",
    "                    'height' : _int64_feature(h),\n",
    "                    'width' : _int64_feature(w),\n",
    "                    'channel' : _int64_feature(ch),\n",
    "                    'depth_raw' : _bytes_feature(depth_raw),\n",
    "                    'image_raw' : _bytes_feature(image_raw)\n",
    "                }))\n",
    "        writer.write(example.SerializeToString()) # Serialize\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_sets(data_sets):\n",
    "    \"\"\"divide data_sets into train and test sets.\"\"\"\n",
    "    train_sets={}\n",
    "    test_sets={}\n",
    "    \n",
    "    train_idx=np.random.choice(1449, 795, replace=False)\n",
    "    test_idx=[]\n",
    "    for i in range(1449):\n",
    "        if i not in train_idx:\n",
    "            test_idx.append(i)\n",
    "    train_idx = list(train_idx)\n",
    "    train_idx.sort()\n",
    "    test_idx.sort()\n",
    "    \n",
    "    train_sets['images'] = data_sets['images'][list(train_idx)]\n",
    "    train_sets['depths'] = data_sets['depths'][list(train_idx)]\n",
    "    test_sets['images'] = data_sets['images'][test_idx]\n",
    "    test_sets['depths'] = data_sets['depths'][test_idx]\n",
    "    \n",
    "    return train_sets, test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_process(imgs, dpts):\n",
    "    def single(img, dpt):\n",
    "        s = random.uniform(0.7, 1.3)\n",
    "        tx = random.uniform(-40, 40); ty = random.uniform(-50, 50)\n",
    "        if s>1.0:\n",
    "            r = random.uniform(-30, 30)\n",
    "        else:\n",
    "            r = random.uniform(0,0)\n",
    "        # scale\n",
    "        aug = ima.Affine(scale=s)\n",
    "        image = aug.augment_image(img); depth = aug.augment_image(dpt)\n",
    "        depth = depth/s\n",
    "        # translation\n",
    "        aug2 = ima.Affine(translate_px={'x':int(tx), 'y':int(ty)})\n",
    "        image = aug2.augment_image(image); depth = aug2.augment_image(depth)\n",
    "        # rotation\n",
    "        aug3 = ima.Affine(rotate=r)\n",
    "        image = aug3.augment_image(image); depth = aug3.augment_image(depth)\n",
    "        # crop\n",
    "        #aug4 = ima.Crop(percent=(0.25), keep_size=False)\n",
    "        #image = aug4.augment_image(image); depth = aug4.augment_image(depth)\n",
    "        return image, depth\n",
    "    \n",
    "    raw_width = 640\n",
    "    raw_height=480\n",
    "    \n",
    "    image, depth = single(imgs[0], dpts[0])\n",
    "    images = np.reshape(image, (1, raw_height, raw_width, 3)) \n",
    "    depths = np.reshape(depth, (1, raw_height, raw_width, 1))\n",
    "    for i in range(len(imgs) - 1):\n",
    "        im, de = single(imgs[i+1], dpts[i+1])\n",
    "        im = np.reshape(im, (1, raw_height, raw_width, 3)); de = np.reshape(de, (1, raw_height, raw_width, 1))\n",
    "        images=np.concatenate((images, im), axis=0); depths=np.concatenate((depths, de), axis=0)\n",
    "    \n",
    "    return images, depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sets = read_mat( './NYU_depthV2/nyu_depth_v2_labeled.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing...\n"
     ]
    }
   ],
   "source": [
    "# Divide data sets into train and test sets.\n",
    "print ('dividing...')\n",
    "train_sets, test_sets = divide_sets(data_sets)\n",
    "del data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting test_sets, and raw_train sets\n",
    "convert_to(test_sets, 'test')\n",
    "convert_to(train_sets, 'train')\n",
    "del test_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
